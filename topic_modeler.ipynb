{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modeler.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAXRr1s2F9ACNPxTa/5Hmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LizzyZhang-tutu/NLP_Learning/blob/master/topic_modeler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4n-FC5-SHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "03dd1b5a-61f5-45dd-f638-2aaf0b5efe89"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim import models, corpora\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load input data\n",
        "def load_data(input_file):\n",
        "    data = []\n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            data.append(line[:-1])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Processor function for tokenizing, removing stop \n",
        "# words, and stemming\n",
        "def process(input_text):\n",
        "    # Create a regular expression tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    # Create a Snowball stemmer \n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    # Get the list of stop words \n",
        "    stop_words = stopwords.words('english')\n",
        "    \n",
        "    # Tokenize the input string\n",
        "    tokens = tokenizer.tokenize(input_text.lower())\n",
        "\n",
        "    # Remove the stop words \n",
        "    tokens = [x for x in tokens if not x in stop_words]\n",
        "    \n",
        "    # Perform stemming on the tokenized words \n",
        "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
        "\n",
        "    return tokens_stemmed\n",
        "    \n",
        "if __name__=='__main__':\n",
        "    # Load input data\n",
        "    data = load_data('data.txt')\n",
        "\n",
        "    # Create a list for sentence tokens\n",
        "    tokens = [process(x) for x in data]\n",
        "\n",
        "    # Create a dictionary based on the sentence tokens \n",
        "    dict_tokens = corpora.Dictionary(tokens)\n",
        "    # Create a document-term matrix\n",
        "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
        "\n",
        "    # Define the number of topics for the LDA model\n",
        "    num_topics = 2\n",
        "\n",
        "    # Generate the LDA model \n",
        "    ldamodel = models.ldamodel.LdaModel(doc_term_mat, \n",
        "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
        "\n",
        "    num_words = 5\n",
        "    print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
        "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "        print('\\nTopic', item[0])\n",
        "\n",
        "        # Print the contributing words along with their relative contributions \n",
        "        list_of_strings = item[1].split(' + ')\n",
        "        print(list_of_strings)\n",
        "        for text in list_of_strings:\n",
        "            weight = text.split('*')[0]\n",
        "            word = text.split('*')[1]\n",
        "            print(word, '==>', str(round(float(weight) * 100, 2)) + '%')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "Top 5 contributing words to each topic:\n",
            "\n",
            "Topic 0\n",
            "['0.053*\"empir\"', '0.038*\"time\"', '0.038*\"histor\"', '0.038*\"peopl\"', '0.038*\"expand\"']\n",
            "\"empir\" ==> 5.3%\n",
            "\"time\" ==> 3.8%\n",
            "\"histor\" ==> 3.8%\n",
            "\"peopl\" ==> 3.8%\n",
            "\"expand\" ==> 3.8%\n",
            "\n",
            "Topic 1\n",
            "['0.034*\"mathemat\"', '0.024*\"europ\"', '0.024*\"cultur\"', '0.024*\"structur\"', '0.024*\"set\"']\n",
            "\"mathemat\" ==> 3.4%\n",
            "\"europ\" ==> 2.4%\n",
            "\"cultur\" ==> 2.4%\n",
            "\"structur\" ==> 2.4%\n",
            "\"set\" ==> 2.4%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}